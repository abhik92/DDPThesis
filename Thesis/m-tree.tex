
In this chapter, we explain the M-tree data structure along with an unique indexing process which builds on the base structure of the metric tree. We explore an alternate indexing technique as well. Later we describe the range search query procedure in detail and study a dimensionality reduction technique to help overcome the high dimensionality and sparsity of the data-sets. \\


\section{M-tree}
M-tree, also known as the Metric tree is a tree data structure constructed using a metric distance measure, and which relies on the triangle inequality for efficient range search queries. Similar to other tree data structures, an M-tree data structure has Leaf Nodes and non-Leaf nodes. Every non-leaf node has a pointer to its parent node, a pointer to its sub-tree, its object information and a covering radius denoting maximum distance of a node to any node in its sub-tree. Every leaf node keeps a pointer to its parent and object information.\\

The M-tree data structure compartments the objects into nodes, which define regions of the metric space. The maximum capacity of the M-tree is M. For each database object to be indexed, there is an entry $O_j = [ O_j, id(O_j), dist(O_j , P(O_j )) ]$
in some leaf node.  $O_j$ stores the object information, mainly the data point feature values or dimension coordinates, id($O_j$) stores the id of the object and $dist(O_j , P(O_j ))$ stores the distance of it to its parent object. \\

\begin{figure}[ht]	
\centering
\includegraphics[width=0.7 \columnwidth]{img/mtree.jpg}
\caption{M-tree: Structure overview}
\label{fig:4.0}
\end{figure}

A non-leaf node $O_r$ stores an object information as well as a covering radius $r(O_r)$ and a pointer to a sub-tree i.e. entry $O_r=[ O_r, ptr(T(O_r)), r(O_r), d(O_r, P(O_r)) ]$. One property satisfied is that every object $O_j$ stored in a sub-tree and rooted at router object $O_r$ is at at-most $r(O_r)$ distance to it i.e. $d(O_j , O_r) â‰¤ r(O_r)$. M-tree organizes the metric space into a set of, possibly overlapping, regions, to which the same principle is recursively applied.\\




\section{Information in our data structure}

We use a modified data structure of the M-tree. We record the following information in each node of the our modified M-tree: the object identifier of the pivot, i.e fingerprint information of the pivot fingerprint; the pivot also stores a pointer to its children i.e a pointer to a set of fingerprints; a double value which is the distance of the farthest child in its sub-tree and a boolean value which signifies if the pivot is an outlier pivot. We do not require a parent pointer. And the size of each node is determined by the number of pivots chosen at each step of our indexing technique and the base outlier size limit, which are explained later. To summarize, the following information is stored in each entry of a node in our tree:

\begin{enumerate}
	\item Object identifier $p_i$
	\item Pointer to sub-tree $S_i$
	\item Farthest child i.e. Covering radius $r_i$
	\item Outlier pivot boolean variable
\end{enumerate}

\section{Baseline Indexing approach}
In the baseline indexing approach, we are partitioning the data-set into groups using pivots which enables us to exploit the triangle inequality. The choice of pivots in the baseline approach is done randomly. The number of pivots is determined by two input parameters viz. the number of random pivots chosen at each step and the minimum size of the outlier set allowed at the lowest level. This has been formally explained in the algorithm below. \\

\begin{figure}[ht]	
\centering
\includegraphics[width=0.7 \columnwidth]{img/image0a.jpg}
\caption{M-tree: Assigning points to a cluster after the pivoting step of the M-tree indexing process}
\label{fig:4.1}
\end{figure}

\begin{figure}[ht]	
\centering
\includegraphics[width=0.7 \columnwidth]{img/image0b.jpg}
\caption{M-tree: Finding the outlier set in the M-tree index structure}
\label{fig:4.2}
\end{figure}
\begin{enumerate}

\item Select the given number of random pivots (lets say $p$) from the database of chemical compounds. The number of random pivots selected in this step is a parameter to our experiment and has been varied across different values.

\item After choosing pivots we assign every other chemical compound in the database to one of the pivots based on the similarity to the pivots. A chemical compound is assigned to the pivot which is nearest to it i.e. the pivot with which it shares the highest Min-Max similarity. This is described in \autoref{fig:4.1}.

\item The next step involves ranking all chemical compounds (which were assigned to a corresponding pivot) in order of their similarity to their closest pivot. 

\item We compute the median similarity (each one to its closest pivot) among all the chemical compounds in this database and note it as the outlier cut-off.

\item All the chemical compounds with similarity less than the cut-off similarity are marked as outliers in this step.

\item The outliers are unassigned from the respective pivot sets and assigned to the "outlier" pivot set. This has been described in \autoref{fig:4.2}.

\item We recursively apply this technique (from step 1 to step 6) on the outlier set until the outlier set size reaches a base limit ($o$), which is also a parameter to our algorithm. Once the size of the outlier set falls below $o$, we terminate this indexing process. \\

\end{enumerate}


\section{Alternate indexing approach}

We tried an alternate approach where instead of choosing outliers we recursively apply the technique of assigning a closest pivot on each set. In simpler words, we cluster at each step and try to create numerous clusters among each cluster recursively. In each cluster we choose pivots again and try to assign each of the points to the pivots until we cannot choose more pivots. We can formally explain it as follows.\\


\begin{enumerate}

\item Select the given number of random pivots ($p$) from the set. The number of random pivots chosen at this step is again a parameter to the experiment as previously mentioned.

\item The second step is similar to the earlier indexing technique. After selecting pivots we assign every other chemical compound in the database to one of the pivots based on the similarity to the pivots, just as before.
 
\item Instead of choosing outliers now, we work with the same sets and recursively apply this technique (steps 1 and 2) on the respective sets until the corresponding set sizes reaches a base limit .

\item The set size limit ($o$) is fixed as the number of pivots chosen at each step ($p$) i.e. the method is applied recursively on the corresponding sets until the size of the set is lower than the pivot set size disabling us from selecting pivot sets. \\

\end{enumerate}

Through experiments conducted on the PubChem data-set (which will be described later), we concluded that our previous indexing technique was more effective than this indexing method. Hence we will be using the earlier technique for all experimental evaluations. A reason for this indexing technique failing to provide us with better results, is that the tree was becoming imbalanced, causing the height of the tree to increase as well as the querying time.


\begin{figure}[ht]	
\centering
\includegraphics[width=1 \columnwidth]{img/image0d.jpg}
\caption{M-tree: Tree structure Step 1}
\label{fig: step1}
\end{figure}


\begin{figure}[ht]	
\centering
\includegraphics[width=1 \columnwidth]{img/image0e.jpg}
\caption{M-tree: Tree structure - After final step}
\label{fig: final step}
\end{figure}



\section{Selecting pivots in the indexing process}

Instead of randomly selecting pivots at every step we use a Max-Min distance approach where we try to maximize the minimum distance between any two pivot nodes chosen. The idea is that the clusters (with the pivots as cluster centre), hence formed, would be spread out. The procedure can be explained as below:

\begin{enumerate}
	\item Select a random point.
	\item The second point is selected in such a manner, that it is farthest from the point selected first. This will require a full database scan.
	\item The third point is chosen such that its minimum distance to either of the previous two pivots is maximized.
	\item This process continues. At the $i^{th}$ iteration, the $i^{th}$ pivot point is chosen in a fashion such that the minimum of its distance to the previous i-1 points is maximized.
	\item This procedure is repeated till we choose $p$ points where $p$ is the number of pivots to be chosen.\\

\end{enumerate} 

We can observe that this is a very expensive procedure since we are scanning the entire database at every step. We would need to compute an all pair similarity initially. At the $i^{th}$ step, the remaining $n-i+1$ points in the database need to compute its minimum distance to the $i-1$ points already present in pivot step. Finally select the point among the $n-i+1$ points with the maximum computed distance. The computation complexity of this pivot selection method is of the order as shown below (Here $n$ is the size of the entire database and $p$ is the number of pivots to be chosen at each step of the indexing process):\\
	\begin{equation}
	T(n)=  n^2 + \sum \limits_{i=1}^{p}( (n-i+1)(i-1)  + n-i+1) 
	\end{equation}
	\begin{equation}
	T(n)=O(n^2 + np^2)
	\end{equation}

We use a sampling technique where we randomly sample a subset of the database and apply the technique described above on the subset. We used a subset size of $\frac{1}{10^{th}}$ the size of the database (chosen empirically), with approximately about 20,000 points in the subset. Since we have a fixed set of points (let this number be k), our computation time complexity for selecting the pivots decreases as shown below: 

	\begin{equation}
	T(n)=  k^2 + \sum \limits_{i=1}^{p}( (k-i+1)(i-1)  + k-i+1) 
	\end{equation}
	\begin{equation}
	T(n)=O(k^2 + kp^2)
	\end{equation}



\section{Range Search Querying}

Given a query chemical fingerprint $q$ and a distance threshold $\theta$ we want to find the set of chemical fingerprints which satisfy the conditions of the range search query. We exploit the triangle inequality for the same. The basic idea is to be able to prune sub-trees based on the covering radius of the pivot of the sub-tree and the distance of the query to the pivot. The procedure for the range search querying can be described by the following steps:\\

\begin{enumerate}
	\item Let the query fingerprint be $q$ and the fingerprint pivot be $p_i$ of sub-tree $S_i$. We can calculate the maximum distance of any node in $S_i$ from q.(We start with the root of the tree as $p_i$).
	
	\item Let the covering radius of pivot $p_i$ be $r_i$. Hence the maximum distance of any node in $S_i$ will be $dist(q,p_i)$ + $r_i$. Similarly the minimum distance of any node in $S_i$ is $max(dist(q,p_i)$ - $r_i, 0)$. 
	
	\item Hence we can compute the range of the distance of any node in $S_i$ to q. 
	
	\item  If the upper bound of the range or the maximum distance is lesser than the threshold distance $\theta$, we can add all the nodes of the sub-tree $S_i$ to our resultant set.
	
	\item If the lower bound of the range is greater than the threshold distance $\theta$, we can prune the sub-tree $S_i$, since we can say with certainty that the distance of every node in the sub-tree $S_i$ to the query point is greater than the threshold $\theta$.
	
	\item If there is an intersection in the intervals we recursively apply this technique on the second level of children in the sub-tree $S_i$ until we reach a leaf node. We make each of the children of the root of the sub-tree $S_i$ as the new $p_i$ and repeat the steps in each of the corresponding sub-trees.
	
\end{enumerate}



\section{Lipschitz Embedding}

The performance of the M-tree is limited by the fact that the data is very sparse. This results in loose clusters being formed during the indexing phase. To improve upon this we performed a dimensionality reduction of the data into another space using Lipschitz embedding. Lipschitz embedding results in a contractive mapping. Let us define the basic terms before we can explain how to fit the embedding process into our indexing procedure.\\

\begin{dfn}
Embedding is the transformation from one space and distance function to another space and distance function. (The new space is generally a vector and the new distance function is, more often than not, Euclidean or some other $L_p$ norm distance function).
\end{dfn}


\begin{dfn}
Lipschitz embedding is the embedding, or in simpler words, dimensionality reduction of the objects of
a database D with metric distance d onto a k-dimensional feature vector space, as described in detail below.  
\end{dfn}

The Lipschitz embedding procedure can be formally explained as follows:\\
\begin{enumerate}
	\item Choose k subsets of D.
	\item Each subset $A_i$ is a reference set.
	\item Let the distance of object $p_j$ to set $A_i$ be defined as:\\
	\begin{equation}
	 d(p_j,A_i)= min_{a\epsilon A_{i}} d(p_j,a) 
	\end{equation}

	\item The new feature vector f($p_j$) i.e. the mapping of the object $p_j$ in the new space, is then defined as:\\
	\begin{equation}
	\label{lip1}
	f(p_j)= {\frac{d(p_j,A_1)}{k},\frac{d(p_j,A_2)}{k}...\frac{d(p_j,A_k)}{k}} 
	\end{equation}

	\item We define the new distance $d'(f(p_j),f(p_k))$ as :\\
	\begin{equation}
	\label{lip2}
	d'(f(p_j),f(p_k)) =\frac{1}{k} \sum \limits_{i=1}^{k} | d(p_j,A_i)-d(p_k,A_i)| 
	\end{equation}

	Note that the new distance measure is the $L_1$ norm in the new space.
	
	\begin{equation}
	\label{eq:L1}
	L_1(f(p_x),f(p_y)) ~~= \sum \limits_{i=1}^{k} | \frac{d(p_x,A_i)}{k}-\frac{d(p_y,A_i)}{k} | ~~=  \frac{1}{k} \sum \limits_{i=1}^{k} | d(p_x,A_i)-d(p_y,A_i)|  ~~= d'(f(p_x),f(p_y))
	\end{equation}
\end{enumerate}



\begin{figure}[ht]	
\centering
\includegraphics[width=0.65 \columnwidth]{img/lip.jpg}
\caption{Lipschitz: Contractive mapping reference diagram}
\label{fig: lip}
\end{figure}


\begin{thm}
\label{th0}
A Lipschitz embedding produces a contractive mapping i.e. a Lipschitz embedding of the form $o_i \Rightarrow f(o_i)$ with $f(o_i)$ as defined in \autoref{lip1} and a new distance measure $d'$ as defined in \autoref{lip2} ensures the following:\\

\begin{equation}
	d'(f(o_i),f(o_j)) < d(o_i, o_j)~ \forall ~ i,j
\end{equation} 

\end{thm}

\begin{proof}
	
Consider any arbitrary reference set $A_l$.\\
\begin{equation}
\exists a_1 \in A_l, d(o_1, A_l) = d(o_1, a_1)
\end{equation}
\begin{equation}
\exists a_2 \in A_l, d(o_2, A_l) = d(o_2, a_2)
\end{equation}

as seen in \autoref{fig: lip}.

Now consider $|d(o_1, A_l)-d(o_2, A_l)|$\\

\begin{equation}
\label{der1}
\begin{split}
|d(o_1, A_l)-d(o_2, A_l)| & = |d(o_1, a_1)-d(o_2, a_2)|  \\
 					& = max(d(o_1, a_1)-d(o_2, a_2),d(o_2, a_2)-d(o_1, a_1))\\
 					& \leq  max(d(o_1, a_2)-d(o_2, a_2),d(o_2, a_1)-d(o_1, a_1))\\
 					& \leq d(o_1,o_2)
\end{split}
\end{equation}

Now consider $d'(f(o_i),f(o_j))$ and using \autoref{lip2} and \autoref{der1}\\

\begin{equation}
\label{der2}
\begin{split}
d'(f(o_i),f(o_j)) & = \frac{1}{k} \sum \limits_{l=1}^{k} | d(o_i,A_l)-d(o_j,A_l)| \\
				& \leq \frac{1}{k} \sum \limits_{l=1}^{k} d(o_i,o_j) \\
				& \leq \frac{1}{k} (k d(o_i,o_j)) \\
				& \leq d(o_i,o_j)
\end{split}
\end{equation}

Hence proved that Lipschitz embedding is a contractive mapping.
\end{proof}

\begin{thm}
A Lipschitz embedding of the form $o_i \Rightarrow f(o_i)$ with $f(o_i)$ as defined in \autoref{lip1} and a new distance measure $d'$ as defined in \autoref{lip2} ensures that $d'$ is a metric in the new space.
\end{thm}

\begin{proof}
We have already shown from \autoref{eq:L1} that $d'$ is equivalent to the $L_1$ norm in the new space. Since we know that $L_1$ norm is a metric, it follows the new distance measure $d'$ is also metric in the new space.
\end{proof}



\section{Using Lipschitz Embedding for Range Search }

We have chosen singleton sets as the reference sets in our embedding process for simplicity and to reduce computation. The choice of these points is done such that they are maximally far apart, i.e., the minimum all-pairs distance among the reference sets is maximized similar to the method of choosing pivots in our indexing technique. This criteria of selecting the sets was based on the idea that farthest points can be used as a more ideal representative of the data-set, rather than points close together. Since choice of such an optimal set is NP-hard, we perform a randomized set construction.\\

\begin{enumerate}
	\item Select a random pivot from a sampled set of  data set points.

	\item Add the point to our Reference set.

	\item Find the point farthest from the reference set from the sampled set of points (based on the criteria explained above, of having the largest minimum distance to the already selected reference sets).

	\item Add the point to the reference set.

	\item Repeat step 3-4 until Reference set is full\\
\end{enumerate}


Using Lipschitz embedding we propose the following indexing and range search process:\\
\begin{enumerate}
	\item Embed the data-set using Lipschitz embedding and the pivoting method described above.
	\item Follow the M-tree based indexing process but in this new embedded space.
	\item Carry out the range search to compute the candidate set.  Since from \hyperref[th0]{Theorem 1}, we know that Lipschitz is contractive we are assured to have our candidate set as a super set of the actual range search result .
	\item We do a similarity computation of all the points in the candidate set to the query, in the original space, to verify if the candidate point is not a false positive.
\end{enumerate}

Through experiments performed on the PubChem dataset (which will be described later), we observed that our performance had deteriorated from that of the baseline technique. This is because the candidate set, which we compute in step 3 is very large, causing us to do a lot of unnecessary similarity computation.
